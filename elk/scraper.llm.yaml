scraper:
  folder: scraper/
  lang: 
    python:
      venv: scraper/.venv
      requirements: requirements.txt
  objective:
    Use open-source information about park safety to compile a data set for Elk
  architecture:
    components:
      - page finder
      - downloader
      - parser
      - inserter
  commander runner:
    file: scraper/Justfile
    description: Useful commands are stored here
    required:
      - download:
          runs: downloader.py
          input: output/urls.txt
          ouput: output/html
      - parse:
          runs: parser.py
          input: output/html
          ouput: output/json
      - transform:
          runs: transformer.py
          input: output/json
          output: output/transformed/<date_time_seconds>.json
      - debug-prompt:
          same as: parse
          except: uses prompt debug mode

page finder:
  ## Disable building page finder for now
  ## It is a nice to have. A list of urls can just be provided for now.
  build: false 
  objective:
    Identity trusted webpages on the internet that have accurate park safety information
  output:
    - urls

downloader:
  objective:
    Download HTML webpages to a folder
  input:
    - page finder urls
  output:
    - html documents:
        folder: output/html/<santized_url_name>.html
  requirements:
    - re-runnable:
        If the url in input has already been downloaded, do not download it again
    - graceful error handling:
        One url failing to download should not stop all urls from failing to download
        Failed url should be logged. Because of re-runnable requirement, the original input
        can just be re-run by user.

parser:
  objective:
    Use on-device large language model to transform HTML documents to hazards, locations, and tips.
  input:
    - downloader html documents
  output:
    - standard out:
      - timestamp for any LLM request with corresponding document name
    - json:
        one file per input document: each input document should output a serialized json
        model speficiation: all models are specified in models.llm.yaml
        structure:
          - tips: 
              type: array of Tips
          - hazards:
              type: array of Hazards
          - locations:
              type: array of locations
  large language model:
    access: 
      description: Assume ollama is running locally
      host: 127.0.0.1
      port: 11434
      use python package: ollama
    model: gpt-oss:20b
    prompt:
      - Mark the beginning of the document
      - Insert document
      - Mark the end of the document
      - Explain the elk system to the llm.
      - Explain the required output
      - Explain the hazard, locations, and tip models to the llm.
      - Explain the objective for this query
      - Request JSON defintion
  requirements:
    - re-generate:
        Every time the script is run, if the JSON file is already generated, re-generate it.
    - include presentations:
        If a hazard is present in a mentioned location, a presentation array should be included in the
        hazards object with the location.
    - omit coordinates:
        Coordinates should be omitted if they are not mentioned in the document
    - strip documents:
        description: HTML documents should be stripped of all HTML tags and syntax. Only the textual elements of the document should be included. This will reduce tokenization time for the LLM.
        implementation:
          package: beautiful soup
          include: text on any element in the HTML document
          exclude: code
    - promompt debugging:
        Provide an option so that the entirety of the prompt for the first document will be logged
    - thinking flag:
        When the thinking flag is enabled, output model thinking
        Stream thinking responses

transformer:
  objectives:
    - consolidation: 
        description:
          Across documents, locations, hazards, and presentations may be referenced several times. 
          Location and hazards should be given unique IDs. If the object already exists in the API, the ID from the API should be used.
          Presentations at a location should be consolidated.
        api:
          endpoints: 
            - Location name search defined in api.llm.yaml
            - Hazard name search defined in api.llm.yaml
        requirements:
          - High Level Location Extraction:
              description:
                Match input location names to extract high level locations rather then specific locations
              examples:
                - inputting `Eagle-Holy Cross Ranger District` and `Eagle-Holy Cross District` yields `Eagle-Holy Cross District`
                - inputting `Burned areas within Rocky Mountain National Park` and `Rocky Mountain National Park` yields `Rocky Mountain National Park`
          - Generate UUIDs:
              If an ID does not exist for location generate a UUID as the ID.
    - hydration:
        description:
          Many documents will only reference locations by name without providing coordinates, boundaries, images, or
          google maps id. 
          The transformer should use Google Places API to get coordinates, images, and google maps id.
          The transformer should use Google Geocoding API to get the boundaries
        google:
          key: GOOGLE_MAPS_API_KEY in local.env
          package: googlemaps
          places:
            reference: https://developers.google.com/maps/documentation/places/web-service/text-search
          geocoding:
            reference: https://developers.google.com/maps/documentation/geocoding/requests-geocoding
            prefer bounds over viewport:
                If the geocoding API returns bounds, this should be used for the location boundary.
                Otherwise, use viewport to define the location boundary.
  input:
    - parser json documents
  output:
    - json document:
        locations:
          kind: array of locations
          presentations:
            kind: array of presentations at location
        hazards:
          kind: array of hazards

inserter:
  ## We will build this after we get the downloader and the parser are working
  build: false
  objective: 
    Insert json files into the api database
  